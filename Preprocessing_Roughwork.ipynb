{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nltk, io, os\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## read data into dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 381 raw speeches\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Speech</th>\n",
       "      <th>Speaker</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1960-06-29_8-102</td>\n",
       "      <td>The Taoiseach</td>\n",
       "      <td>The Minister's focus is on improving the healt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1960-06-29_8-116</td>\n",
       "      <td>Mr. T.F. O'Higgins</td>\n",
       "      <td>That must be a very infrequent occurrence.\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1971-06-09_23-2</td>\n",
       "      <td>Parliamentary Secretary to the Minister for th...</td>\n",
       "      <td>Is Ã© an cuspÃ³ir atÃ¡ ag an Rialtas nÃ¡ lÃ¡nf...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2015-01-27_20-105</td>\n",
       "      <td>Deputy Peter Mathews</td>\n",
       "      <td>Why do we not remember them here?\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1971-06-09_19-17</td>\n",
       "      <td>The Taoiseach</td>\n",
       "      <td>Yesterday's ceremony was a moving, fitting and...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Speech                                            Speaker  \\\n",
       "0   1960-06-29_8-102                                      The Taoiseach   \n",
       "1   1960-06-29_8-116                                 Mr. T.F. O'Higgins   \n",
       "2    1971-06-09_23-2  Parliamentary Secretary to the Minister for th...   \n",
       "3  2015-01-27_20-105                               Deputy Peter Mathews   \n",
       "4   1971-06-09_19-17                                      The Taoiseach   \n",
       "\n",
       "                                                Text  \n",
       "0  The Minister's focus is on improving the healt...  \n",
       "1       That must be a very infrequent occurrence.\\n  \n",
       "2  Is Ã© an cuspÃ³ir atÃ¡ ag an Rialtas nÃ¡ lÃ¡nf...  \n",
       "3                Why do we not remember them here?\\n  \n",
       "4  Yesterday's ceremony was a moving, fitting and...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datapath = './Speeches/'\n",
    "df_raw = pd.DataFrame([],columns=['Speech','Speaker','Text'])\n",
    "\n",
    "for root, directories, files in os.walk(datapath):\n",
    "    for filename in files:\n",
    "        speech = os.path.splitext(filename)[0] # to get rid of .txt\n",
    "        speech = speech[7:] # to remove Speech-\n",
    "        filepath = os.path.join(root, filename)\n",
    "        f = open(filepath,\"r\",encoding='latin') \n",
    "        content = f.read()\n",
    "        f.close()\n",
    "        content = content.split(\"\\n\\n\",1) # max split = 1\n",
    "        speaker = content[0]\n",
    "        text = content[1:]\n",
    "        text = ''.join(text) # list to string\n",
    "\n",
    "        pieces = {'Speech': speech,'Speaker':speaker, 'Text':text}\n",
    "        df_raw = df_raw.append(pieces,ignore_index=True)\n",
    "print(\"Read %d raw speeches\" % len(df_raw))\n",
    "\n",
    "df_raw.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize Content column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordExp = r'\\w+' # this pattern finds all the words\n",
    "tokenizer = RegexpTokenizer(wordExp)\n",
    "\n",
    "tokensList = [] # this is a list of lists. Each list contains the tokens of a document.\n",
    "\n",
    "for content in df_raw['Text']:\n",
    "    token_words = tokenizer.tokenize(content) #tokenize all words in the document\n",
    "    tokensList.append(token_words) #add this list to tokensList"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decapitalisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decapitalise(list_of_tokenLists):\n",
    "    newTokenList = [] #this will store the new list\n",
    "    \n",
    "    for tokens in list_of_tokenLists: #for each list in the big list\n",
    "        \n",
    "        decapitalised = [] #this will store a list of decapitalised tokens from a single doc\n",
    "        for word in tokens: #for each word in the list\n",
    "            if not word.isupper(): #don't decapitalise if the whole word is in uppercase anyway - avoids decapitalising acronyms like US, LA, ID \n",
    "                decapitalised.append(word.lower()) # change to lower case\n",
    "            else:\n",
    "                decapitalised.append(word)\n",
    "        newTokenList.append(decapitalised) #add decapitalised list to overall list\n",
    "        \n",
    "    return newTokenList"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(list_of_tokenLists):\n",
    "    stopword_list = stopwords.words('english') #import default nltk stopwords\n",
    "    newTokenList = [] #this will store the new list\n",
    "    \n",
    "    for tokens in list_of_tokenLists: #for each list in the big list\n",
    "        notStop = [] #this will store a list of non-stopword tokens from a single doc\n",
    "        for word in tokens: #for each word in the list\n",
    "            if word not in stopword_list: #if word is not a stopword, append it\n",
    "                notStop.append(word)\n",
    "        newTokenList.append(notStop) #append newlist to the overall list\n",
    "        \n",
    "    return newTokenList"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove numbers\n",
    "* This removes numbers from the token list  \n",
    "* Does not remove words that contain numbers (e.g): 70th"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_numbers(list_of_tokenLists):\n",
    "    newTokenList = [] #this will store the new list\n",
    "    \n",
    "    for tokens in list_of_tokenLists: #for each list in the big list\n",
    "        \n",
    "        noNumbers= [] #this will store a list of tokens from a single doc\n",
    "        for word in tokens: #for each word in the list#\n",
    "            if not word.isdigit():\n",
    "                noNumbers.append(word) # remove number\n",
    "        newTokenList.append(noNumbers)\n",
    "        \n",
    "    return newTokenList"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def punctuation(list_of_tokenLists):\n",
    "    newTokenList = [] #this will store the new list\n",
    "    \n",
    "    for tokens in list_of_tokenLists: #for each list in the big list\n",
    "        \n",
    "        depunctuated = [] #this will store a list of tokens from a single doc\n",
    "        for word in tokens: #for each word in the list\n",
    "            depunctuated.append(word.translate(string.punctuation)) # remove punctuation\n",
    "        newTokenList.append(depunctuated)\n",
    "    return newTokenList"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expand Contractions\n",
    "* Removing punctuation needs to be applied before expanding contractions. Otherwise contractions with hyphens and apostrophes won't be found.\n",
    "* Contractions are shortened version of words or syllables. \n",
    "* Converting each contraction to its expanded, original form helps with text standardization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expandContractions(inputList):\n",
    "    contractions = { \n",
    "    \"ain't\": \"am not / are not / is not / has not / have not\",\n",
    "    \"aren't\": \"are not / am not\",\n",
    "    \"can't\": \"cannot\",\n",
    "    \"can't've\": \"cannot have\",\n",
    "    \"'cause\": \"because\",\n",
    "    \"could've\": \"could have\",\n",
    "    \"couldn't\": \"could not\",\n",
    "    \"couldn't've\": \"could not have\",\n",
    "    \"didn't\": \"did not\",\n",
    "    \"doesn't\": \"does not\",\n",
    "    \"don't\": \"do not\",\n",
    "    \"hadn't\": \"had not\",\n",
    "    \"hadn't've\": \"had not have\",\n",
    "    \"hasn't\": \"has not\",\n",
    "    \"haven't\": \"have not\",\n",
    "    \"he'd\": \"he had / he would\",\n",
    "    \"he'd've\": \"he would have\",\n",
    "    \"he'll\": \"he will\",\n",
    "    \"he'll've\": \"he will have\",\n",
    "    \"he's\": \"he is\",\n",
    "    \"how'd\": \"how did\",\n",
    "    \"how'd'y\": \"how do you\",\n",
    "    \"how'll\": \"how will\",\n",
    "    \"how's\": \"how has / how is / how does\",\n",
    "    \"I'd\": \"I had / I would\",\n",
    "    \"I'd've\": \"I would have\",\n",
    "    \"I'll\": \"I will\",\n",
    "    \"I'll've\": \"I will have\",\n",
    "    \"I'm\": \"I am\",\n",
    "    \"I've\": \"I have\",\n",
    "    \"isn't\": \"is not\",\n",
    "    \"it'd\": \"it had / it would\",\n",
    "    \"it'd've\": \"it would have\",\n",
    "    \"it'll\": \"it shall / it will\",\n",
    "    \"it'll've\": \"it will have\",\n",
    "    \"it's\": \"it has / it is\",\n",
    "    \"let's\": \"let us\",\n",
    "    \"ma'am\": \"madam\",\n",
    "    \"mayn't\": \"may not\",\n",
    "    \"might've\": \"might have\",\n",
    "    \"mightn't\": \"might not\",\n",
    "    \"mightn't've\": \"might not have\",\n",
    "    \"must've\": \"must have\",\n",
    "    \"mustn't\": \"must not\",\n",
    "    \"mustn't've\": \"must not have\",\n",
    "    \"needn't\": \"need not\",\n",
    "    \"needn't've\": \"need not have\",\n",
    "    \"o'clock\": \"of the clock\",\n",
    "    \"oughtn't\": \"ought not\",\n",
    "    \"oughtn't've\": \"ought not have\",\n",
    "    \"shan't\": \"shall not\",\n",
    "    \"sha'n't\": \"shall not\",\n",
    "    \"shan't've\": \"shall not have\",\n",
    "    \"she'd\": \"she had / she would\",\n",
    "    \"she'd've\": \"she would have\",\n",
    "    \"she'll\": \"she will\",\n",
    "    \"she'll've\": \"she will have\",\n",
    "    \"she's\": \"she has / she is\",\n",
    "    \"should've\": \"should have\",\n",
    "    \"shouldn't\": \"should not\",\n",
    "    \"shouldn't've\": \"should not have\",\n",
    "    \"so've\": \"so have\",\n",
    "    \"so's\": \"so as / so is\",\n",
    "    \"that'd\": \"that would / that had\",\n",
    "    \"that'd've\": \"that would have\",\n",
    "    \"that's\": \"that has / that is\",\n",
    "    \"there'd\": \"there had / there would\",\n",
    "    \"there'd've\": \"there would have\",\n",
    "    \"there's\": \"there has / there is\",\n",
    "    \"they'd\": \"they had / they would\",\n",
    "    \"they'd've\": \"they would have\",\n",
    "    \"they'll\": \"they will\",\n",
    "    \"they'll've\": \"they will have\",\n",
    "    \"they're\": \"they are\",\n",
    "    \"they've\": \"they have\",\n",
    "    \"to've\": \"to have\",\n",
    "    \"wasn't\": \"was not\",\n",
    "    \"we'd\": \"we had / we would\",\n",
    "    \"we'd've\": \"we would have\",\n",
    "    \"we'll\": \"we will\",\n",
    "    \"we'll've\": \"we will have\",\n",
    "    \"we're\": \"we are\",\n",
    "    \"we've\": \"we have\",\n",
    "    \"weren't\": \"were not\",\n",
    "    \"what'll\": \" what will\",\n",
    "    \"what'll've\": \"what will have\",\n",
    "    \"what're\": \"what are\",\n",
    "    \"what's\": \"what has / what is\",\n",
    "    \"what've\": \"what have\",\n",
    "    \"when's\": \"when has / when is\",\n",
    "    \"when've\": \"when have\",\n",
    "    \"where'd\": \"where did\",\n",
    "    \"where's\": \"where has / where is\",\n",
    "    \"where've\": \"where have\",\n",
    "    \"who'll\": \"who will\",\n",
    "    \"who'll've\": \"who will have\",\n",
    "    \"who's\": \"who has / who is\",\n",
    "    \"who've\": \"who have\",\n",
    "    \"why's\": \"why has / why is\",\n",
    "    \"why've\": \"why have\",\n",
    "    \"will've\": \"will have\",\n",
    "    \"won't\": \"will not\",\n",
    "    \"won't've\": \"will not have\",\n",
    "    \"would've\": \"would have\",\n",
    "    \"wouldn't\": \"would not\",\n",
    "    \"wouldn't've\": \"would not have\",\n",
    "    \"y'all\": \"you all\",\n",
    "    \"y'all'd\": \"you all would\",\n",
    "    \"y'all'd've\": \"you all would have\",\n",
    "    \"y'all're\": \"you all are\",\n",
    "    \"y'all've\": \"you all have\",\n",
    "    \"you'd\": \"you had / you would\",\n",
    "    \"you'd've\": \"you would have\",\n",
    "    \"you'll\": \"you will\",\n",
    "    \"you'll've\": \"you will have\",\n",
    "    \"you're\": \"you are\",\n",
    "    \"you've\": \"you have\",\n",
    "    }\n",
    "    #for every word in the input text\n",
    "    for list_of_tokens in inputList:\n",
    "        for word in list_of_tokens:\n",
    "            # if the word is in our contractions dictionary replace it with the expanded version.\n",
    "            if (word.lower() in contractions):\n",
    "                inputList = inputList.replace(word, contractions[word.lower()])\n",
    "            #if the word contains a hyphen, replace the hyphen with a space leaving two words\n",
    "            if (\"-\" in word):\n",
    "                inputList = inputList.replace(word, word.replace(\"-\", \" \"))\n",
    "    return (inputList)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize(list_of_tokenLists):\n",
    "    newTokenList = [] #this will store the new list\n",
    "    wordnet_lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    for tokens in list_of_tokenLists:\n",
    "        tempLemmatized = []\n",
    "        for word in tokens:\n",
    "            tempLemmatized.append(wordnet_lemmatizer.lemmatize(word))\n",
    "        newTokenList.append(tempLemmatized)\n",
    "    return newTokenList"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stemming(list_of_tokenLists):\n",
    "    newTokenList = [] #this will store the new list\n",
    "    stemmer = PorterStemmer()\n",
    "    \n",
    "    for tokens in list_of_tokenLists:\n",
    "        tempStemmed = []\n",
    "        for word in tokens:\n",
    "            tempStemmed.append(stemmer.stem(word))\n",
    "        newTokenList.append(tempStemmed)\n",
    "    return newTokenList"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply above functions to create \"cleaned\" tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(list_of_tokenLists):\n",
    "    list_of_tokenLists = decapitalise(list_of_tokenLists) #decapitalise\n",
    "    list_of_tokenLists = remove_stopwords(list_of_tokenLists) #remove stopwords\n",
    "    list_of_tokenLists = remove_numbers(list_of_tokenLists) #remove numbers\n",
    "    list_of_tokenLists = punctuation(list_of_tokenLists) #remove punctuation\n",
    "    list_of_tokenLists = expandContractions(list_of_tokenLists) #expand contractions\n",
    "    list_of_tokenLists = stemming(list_of_tokenLists) # stemming\n",
    "    return list_of_tokenLists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens Before: \n",
      "['The', 'Minister', 's', 'focus', 'is', 'on', 'improving', 'the', 'health', 'services', 'In', 'respect', 'of', 'Deputy', 'Mathews', 'I', 'might', 'say', 'that', 'the', 'President', 'of', 'Ireland', 'Ã', 'achtarÃ', 'n', 'na', 'hÃ', 'ireann', 'represented', 'all', 'our', 'people', 'in', 'the', 'Mansion', 'House', 'on', 'Sunday', 'He', 'represented', 'everybody']\n",
      "------------\n",
      "Tokens After: \n",
      "['minist', 'focu', 'improv', 'health', 'servic', 'respect', 'deputi', 'mathew', 'I', 'might', 'say', 'presid', 'ireland', 'Ã', 'achtarã', 'n', 'na', 'hã', 'ireann', 'repres', 'peopl', 'mansion', 'hous', 'sunday', 'repres', 'everybodi']\n"
     ]
    }
   ],
   "source": [
    "print(\"Tokens Before: \")\n",
    "print(tokensList[0][:50])\n",
    "print(\"------------\")\n",
    "\n",
    "tokensList = clean(tokensList) # clean\n",
    " \n",
    "print(\"Tokens After: \")\n",
    "print(tokensList[0][:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Speech</th>\n",
       "      <th>Speaker</th>\n",
       "      <th>Text</th>\n",
       "      <th>Tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1960-06-29_8-102</td>\n",
       "      <td>The Taoiseach</td>\n",
       "      <td>The Minister's focus is on improving the healt...</td>\n",
       "      <td>[minist, focu, improv, health, servic, respect...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1960-06-29_8-116</td>\n",
       "      <td>Mr. T.F. O'Higgins</td>\n",
       "      <td>That must be a very infrequent occurrence.\\n</td>\n",
       "      <td>[must, infrequ, occurr]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1971-06-09_23-2</td>\n",
       "      <td>Parliamentary Secretary to the Minister for th...</td>\n",
       "      <td>Is Ã© an cuspÃ³ir atÃ¡ ag an Rialtas nÃ¡ lÃ¡nf...</td>\n",
       "      <td>[Ã, cuspã³ir, atã, ag, rialta, nã, lã, nfhosta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2015-01-27_20-105</td>\n",
       "      <td>Deputy Peter Mathews</td>\n",
       "      <td>Why do we not remember them here?\\n</td>\n",
       "      <td>[rememb]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1971-06-09_19-17</td>\n",
       "      <td>The Taoiseach</td>\n",
       "      <td>Yesterday's ceremony was a moving, fitting and...</td>\n",
       "      <td>[yesterday, ceremoni, move, fit, appropri, rec...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Speech                                            Speaker  \\\n",
       "0   1960-06-29_8-102                                      The Taoiseach   \n",
       "1   1960-06-29_8-116                                 Mr. T.F. O'Higgins   \n",
       "2    1971-06-09_23-2  Parliamentary Secretary to the Minister for th...   \n",
       "3  2015-01-27_20-105                               Deputy Peter Mathews   \n",
       "4   1971-06-09_19-17                                      The Taoiseach   \n",
       "\n",
       "                                                Text  \\\n",
       "0  The Minister's focus is on improving the healt...   \n",
       "1       That must be a very infrequent occurrence.\\n   \n",
       "2  Is Ã© an cuspÃ³ir atÃ¡ ag an Rialtas nÃ¡ lÃ¡nf...   \n",
       "3                Why do we not remember them here?\\n   \n",
       "4  Yesterday's ceremony was a moving, fitting and...   \n",
       "\n",
       "                                              Tokens  \n",
       "0  [minist, focu, improv, health, servic, respect...  \n",
       "1                            [must, infrequ, occurr]  \n",
       "2  [Ã, cuspã³ir, atã, ag, rialta, nã, lã, nfhosta...  \n",
       "3                                           [rememb]  \n",
       "4  [yesterday, ceremoni, move, fit, appropri, rec...  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_raw['Tokens'] = tokensList\n",
    "df_raw.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Join the cleaned tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Speech</th>\n",
       "      <th>Speaker</th>\n",
       "      <th>Text</th>\n",
       "      <th>Tokens</th>\n",
       "      <th>clean_Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1960-06-29_8-102</td>\n",
       "      <td>The Taoiseach</td>\n",
       "      <td>The Minister's focus is on improving the healt...</td>\n",
       "      <td>[minist, focu, improv, health, servic, respect...</td>\n",
       "      <td>minist focu improv health servic respect deput...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1960-06-29_8-116</td>\n",
       "      <td>Mr. T.F. O'Higgins</td>\n",
       "      <td>That must be a very infrequent occurrence.\\n</td>\n",
       "      <td>[must, infrequ, occurr]</td>\n",
       "      <td>must infrequ occurr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1971-06-09_23-2</td>\n",
       "      <td>Parliamentary Secretary to the Minister for th...</td>\n",
       "      <td>Is Ã© an cuspÃ³ir atÃ¡ ag an Rialtas nÃ¡ lÃ¡nf...</td>\n",
       "      <td>[Ã, cuspã³ir, atã, ag, rialta, nã, lã, nfhosta...</td>\n",
       "      <td>Ã cuspã³ir atã ag rialta nã lã nfhostaã ocht c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2015-01-27_20-105</td>\n",
       "      <td>Deputy Peter Mathews</td>\n",
       "      <td>Why do we not remember them here?\\n</td>\n",
       "      <td>[rememb]</td>\n",
       "      <td>rememb</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1971-06-09_19-17</td>\n",
       "      <td>The Taoiseach</td>\n",
       "      <td>Yesterday's ceremony was a moving, fitting and...</td>\n",
       "      <td>[yesterday, ceremoni, move, fit, appropri, rec...</td>\n",
       "      <td>yesterday ceremoni move fit appropri recognit ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Speech                                            Speaker  \\\n",
       "0   1960-06-29_8-102                                      The Taoiseach   \n",
       "1   1960-06-29_8-116                                 Mr. T.F. O'Higgins   \n",
       "2    1971-06-09_23-2  Parliamentary Secretary to the Minister for th...   \n",
       "3  2015-01-27_20-105                               Deputy Peter Mathews   \n",
       "4   1971-06-09_19-17                                      The Taoiseach   \n",
       "\n",
       "                                                Text  \\\n",
       "0  The Minister's focus is on improving the healt...   \n",
       "1       That must be a very infrequent occurrence.\\n   \n",
       "2  Is Ã© an cuspÃ³ir atÃ¡ ag an Rialtas nÃ¡ lÃ¡nf...   \n",
       "3                Why do we not remember them here?\\n   \n",
       "4  Yesterday's ceremony was a moving, fitting and...   \n",
       "\n",
       "                                              Tokens  \\\n",
       "0  [minist, focu, improv, health, servic, respect...   \n",
       "1                            [must, infrequ, occurr]   \n",
       "2  [Ã, cuspã³ir, atã, ag, rialta, nã, lã, nfhosta...   \n",
       "3                                           [rememb]   \n",
       "4  [yesterday, ceremoni, move, fit, appropri, rec...   \n",
       "\n",
       "                                          clean_Text  \n",
       "0  minist focu improv health servic respect deput...  \n",
       "1                                must infrequ occurr  \n",
       "2  Ã cuspã³ir atã ag rialta nã lã nfhostaã ocht c...  \n",
       "3                                             rememb  \n",
       "4  yesterday ceremoni move fit appropri recognit ...  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joinedTokensList = []\n",
    "\n",
    "for i in tokensList:\n",
    "    joinedTokensList.append(\" \".join(i))\n",
    "df_raw['clean_Text'] = joinedTokensList\n",
    "new_df = df_raw\n",
    "new_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Group by Year, tfidf, NMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Speech</th>\n",
       "      <th>Speaker</th>\n",
       "      <th>Text</th>\n",
       "      <th>Tokens</th>\n",
       "      <th>clean_Text</th>\n",
       "      <th>Year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1960-06-29_8-102</td>\n",
       "      <td>The Taoiseach</td>\n",
       "      <td>The Minister's focus is on improving the healt...</td>\n",
       "      <td>[minist, focu, improv, health, servic, respect...</td>\n",
       "      <td>minist focu improv health servic respect deput...</td>\n",
       "      <td>1960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1960-06-29_8-116</td>\n",
       "      <td>Mr. T.F. O'Higgins</td>\n",
       "      <td>That must be a very infrequent occurrence.\\n</td>\n",
       "      <td>[must, infrequ, occurr]</td>\n",
       "      <td>must infrequ occurr</td>\n",
       "      <td>1960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1971-06-09_23-2</td>\n",
       "      <td>Parliamentary Secretary to the Minister for th...</td>\n",
       "      <td>Is Ã© an cuspÃ³ir atÃ¡ ag an Rialtas nÃ¡ lÃ¡nf...</td>\n",
       "      <td>[Ã, cuspã³ir, atã, ag, rialta, nã, lã, nfhosta...</td>\n",
       "      <td>Ã cuspã³ir atã ag rialta nã lã nfhostaã ocht c...</td>\n",
       "      <td>1971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2015-01-27_20-105</td>\n",
       "      <td>Deputy Peter Mathews</td>\n",
       "      <td>Why do we not remember them here?\\n</td>\n",
       "      <td>[rememb]</td>\n",
       "      <td>rememb</td>\n",
       "      <td>2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1971-06-09_19-17</td>\n",
       "      <td>The Taoiseach</td>\n",
       "      <td>Yesterday's ceremony was a moving, fitting and...</td>\n",
       "      <td>[yesterday, ceremoni, move, fit, appropri, rec...</td>\n",
       "      <td>yesterday ceremoni move fit appropri recognit ...</td>\n",
       "      <td>1971</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Speech                                            Speaker  \\\n",
       "0   1960-06-29_8-102                                      The Taoiseach   \n",
       "1   1960-06-29_8-116                                 Mr. T.F. O'Higgins   \n",
       "2    1971-06-09_23-2  Parliamentary Secretary to the Minister for th...   \n",
       "3  2015-01-27_20-105                               Deputy Peter Mathews   \n",
       "4   1971-06-09_19-17                                      The Taoiseach   \n",
       "\n",
       "                                                Text  \\\n",
       "0  The Minister's focus is on improving the healt...   \n",
       "1       That must be a very infrequent occurrence.\\n   \n",
       "2  Is Ã© an cuspÃ³ir atÃ¡ ag an Rialtas nÃ¡ lÃ¡nf...   \n",
       "3                Why do we not remember them here?\\n   \n",
       "4  Yesterday's ceremony was a moving, fitting and...   \n",
       "\n",
       "                                              Tokens  \\\n",
       "0  [minist, focu, improv, health, servic, respect...   \n",
       "1                            [must, infrequ, occurr]   \n",
       "2  [Ã, cuspã³ir, atã, ag, rialta, nã, lã, nfhosta...   \n",
       "3                                           [rememb]   \n",
       "4  [yesterday, ceremoni, move, fit, appropri, rec...   \n",
       "\n",
       "                                          clean_Text  Year  \n",
       "0  minist focu improv health servic respect deput...  1960  \n",
       "1                                must infrequ occurr  1960  \n",
       "2  Ã cuspã³ir atã ag rialta nã lã nfhostaã ocht c...  1971  \n",
       "3                                             rememb  2015  \n",
       "4  yesterday ceremoni move fit appropri recognit ...  1971  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create new column to contain the Year\n",
    "[n,d] = new_df.shape\n",
    "new_df['Year'] = ['']*n\n",
    "\n",
    "for index, row in new_df.iterrows():\n",
    "    new_df['Year'].iloc[index] = (row['Speech'][:4])\n",
    "    \n",
    "new_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 126 X 1573 TF-IDF-normalized document-term matrix\n",
      "Vocabulary has 1573 distinct terms\n",
      "Checking for ./tfidf/tfidf-1960.txt\n",
      "Created 142 X 1597 TF-IDF-normalized document-term matrix\n",
      "Vocabulary has 1597 distinct terms\n",
      "Checking for ./tfidf/tfidf-1971.txt\n",
      "Created 112 X 972 TF-IDF-normalized document-term matrix\n",
      "Vocabulary has 972 distinct terms\n",
      "Checking for ./tfidf/tfidf-2015.txt\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "empty vocabulary; perhaps the documents only contain stop words",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-819385354670>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;31m# Create sparse NumPy array where the entries are all TF-IDF normalised\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mtfidf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtfidf_vectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocuments\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0;34m\"Created %d X %d TF-IDF-normalized document-term matrix\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtfidf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtfidf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mterms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtfidf_vectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_feature_names\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   1379\u001b[0m             \u001b[0mTf\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0midf\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mweighted\u001b[0m \u001b[0mdocument\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mterm\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1380\u001b[0m         \"\"\"\n\u001b[0;32m-> 1381\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTfidfVectorizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1382\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tfidf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1383\u001b[0m         \u001b[0;31m# X is already a transformed view of raw_documents so\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m    867\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    868\u001b[0m         vocabulary, X = self._count_vocab(raw_documents,\n\u001b[0;32m--> 869\u001b[0;31m                                           self.fixed_vocabulary_)\n\u001b[0m\u001b[1;32m    870\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m_count_vocab\u001b[0;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[1;32m    809\u001b[0m             \u001b[0mvocabulary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocabulary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    810\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 811\u001b[0;31m                 raise ValueError(\"empty vocabulary; perhaps the documents only\"\n\u001b[0m\u001b[1;32m    812\u001b[0m                                  \" contain stop words\")\n\u001b[1;32m    813\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: empty vocabulary; perhaps the documents only contain stop words"
     ]
    }
   ],
   "source": [
    "from sklearn.externals import joblib\n",
    "groups_by_year = new_df.groupby('Year')\n",
    "groupsList = groups_by_year.groups.keys()\n",
    "\n",
    "for group in groupsList: #for each Year\n",
    "    # get documents\n",
    "    documents = groups_by_year.get_group(group)['clean_Text']\n",
    "    \n",
    "    tfidf_vectorizer = TfidfVectorizer() # declare vectorizer object\n",
    "    \n",
    "    # Create sparse NumPy array where the entries are all TF-IDF normalised\n",
    "    tfidf = tfidf_vectorizer.fit_transform(documents) \n",
    "    print( \"Created %d X %d TF-IDF-normalized document-term matrix\" % (tfidf.shape[0], tfidf.shape[1]) )    \n",
    "    terms = tfidf_vectorizer.get_feature_names()\n",
    "    print(\"Vocabulary has %d distinct terms\" % len(terms))\n",
    "    \n",
    "    # Save each tfidf\n",
    "    filepath = \"./tfidf/tfidf-{}.txt\".format(group)\n",
    "    print(\"Checking for %s\" % filepath)\n",
    "    # Check if file has already been downloaded\n",
    "    if not os.path.exists( filepath ):\n",
    "        print(\"Writing %s\" % filepath)\n",
    "        joblib.dump((tfidf,terms), filepath)\n",
    "print(\"Process Complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to display the document topics\n",
    "import numpy as np\n",
    "def get_topics( terms, H, topic_index, top ):\n",
    "    # Reverse sort the values to sort the indices\n",
    "    top_indices = np.argsort( H[topic_index,:] )[::-1]\n",
    "    # Terms for top-ranked indices\n",
    "    top_terms = []\n",
    "    for term_index in top_indices[0:top]:\n",
    "        # Append terms to top_terms\n",
    "        top_terms.append( terms[term_index] )\n",
    "    return top_terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_topics = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing ./tfidf/tfidf-1960.txt\n",
      "Loaded 126 X 1573 document-term matrix\n",
      "1960\n",
      "Topic 01: prison, minist, govern, peopl, would, state, hospit, book, governor, water\n",
      "Topic 02: pleas, order, quiet, deputi, ask, legisl, busi, paper, cheann, lea\n",
      "Topic 03: bill, taoiseach, could, deal, reform, respond, health, matter, vote, call\n",
      "Topic 04: debat, tomorrow, inquiri, someth, state, vote, whip, minist, comhairl, ceann\n",
      "Topic 05: deputi, troy, seat, mathew, resum, sorri, question, floor, rais, must\n",
      "Checking for ./nmf_models/Speech-nmf-model-1960-1.txt\n",
      "Checking for ./nmf_models/Speech-nmf-model-1960-2.txt\n",
      "Checking for ./nmf_models/Speech-nmf-model-1960-3.txt\n",
      "Checking for ./nmf_models/Speech-nmf-model-1960-4.txt\n",
      "Checking for ./nmf_models/Speech-nmf-model-1960-5.txt\n",
      "Processing ./tfidf/tfidf-1971.txt\n",
      "Loaded 142 X 1597 document-term matrix\n",
      "1971\n",
      "Topic 01: bill, taoiseach, could, health, prison, govern, reform, minist, deputi, deal\n",
      "Topic 02: ghaorthaidh, tha, bhfuil, aon, al, leith, gcã³ir, bhã, faoi, tionscal\n",
      "Topic 03: pleas, deputi, order, mathew, seat, resum, troy, quiet, question, feet\n",
      "Topic 04: ag, ar, rã, fã, gaeltarra, atã, gceist, ghaeltacht, cuspã³ir, tã\n",
      "Topic 05: debat, tomorrow, inquiri, someth, state, vote, whip, comhairl, minist, ceann\n",
      "Checking for ./nmf_models/Speech-nmf-model-1971-1.txt\n",
      "Checking for ./nmf_models/Speech-nmf-model-1971-2.txt\n",
      "Checking for ./nmf_models/Speech-nmf-model-1971-3.txt\n",
      "Checking for ./nmf_models/Speech-nmf-model-1971-4.txt\n",
      "Checking for ./nmf_models/Speech-nmf-model-1971-5.txt\n",
      "Processing ./tfidf/tfidf-2015.txt\n",
      "Loaded 112 X 972 document-term matrix\n",
      "2015\n",
      "Topic 01: pleas, order, quiet, deputi, legisl, ask, mathew, lea, cheann, interrupt\n",
      "Topic 02: bill, health, reform, respond, could, govern, minist, water, taoiseach, state\n",
      "Topic 03: deal, taoiseach, order, matter, busi, call, context, rememb, could, relev\n",
      "Topic 04: debat, tomorrow, someth, state, inquiri, minist, vote, comhairl, side, contribut\n",
      "Topic 05: deputi, troy, mathew, seat, resum, sorri, question, floor, rais, must\n",
      "Checking for ./nmf_models/Speech-nmf-model-2015-1.txt\n",
      "Checking for ./nmf_models/Speech-nmf-model-2015-2.txt\n",
      "Checking for ./nmf_models/Speech-nmf-model-2015-3.txt\n",
      "Checking for ./nmf_models/Speech-nmf-model-2015-4.txt\n",
      "Checking for ./nmf_models/Speech-nmf-model-2015-5.txt\n",
      "Process Complete\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "from sklearn import decomposition\n",
    "allfiles = glob.glob('./tfidf/tfidf-*.txt')\n",
    "for file in allfiles:\n",
    "    print(\"Processing %s\" % file)\n",
    "    from sklearn.externals import joblib\n",
    "    (tfidf,terms) = joblib.load( file )\n",
    "    print( \"Loaded %d X %d document-term matrix\" % (tfidf.shape[0], tfidf.shape[1]) )\n",
    "    \n",
    "    nmf_model = decomposition.NMF(n_components=num_topics, init=\"nndsvd\") # use randominitialization\n",
    "    W = nmf_model.fit_transform( tfidf ) # W = matrix that contains the topics discovered from the documents\n",
    "    H = nmf_model.components_ # H = coefficient matrix containing the membership weights for the topics in each document\n",
    "    \n",
    "    # To display the document topics\n",
    "    print(file[14:-4])# To print (e.g.): 1960\n",
    "    topic_words = []\n",
    "    for topic_index in range(num_topics):\n",
    "        topic_words.append( get_topics( terms, H, topic_index, 10 ) )\n",
    "        str_topic_words = \", \".join( topic_words[topic_index] )\n",
    "        print(\"Topic %02d: %s\" % ( topic_index+1, str_topic_words ) )\n",
    "    \n",
    "    # Save topic model\n",
    "    num=0\n",
    "    for i in range(num_topics):\n",
    "        num+=1\n",
    "        filepath = \"./nmf_models/Speech-nmf-model-{}.txt\".format(file[14:-4] + \"-\" + str(num))\n",
    "        print(\"Checking for %s\" % filepath)\n",
    "        # Check if file has already been downloaded\n",
    "        if not os.path.exists( filepath ):\n",
    "            print(\"Writing %s\" % filepath)\n",
    "            joblib.dump((W,H,terms), filepath)\n",
    "print(\"Process Complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
